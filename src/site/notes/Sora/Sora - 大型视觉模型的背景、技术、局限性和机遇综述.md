---
{"dg-publish":true,"permalink":"/Sora/Sora - 大型视觉模型的背景、技术、局限性和机遇综述/"}
---

Sora 是 OpenAI 于 2024 年 2 月发布的文本到视频生成式 AI 模型。可以从文本指令生成逼真或富有想象力的场景视频，并显示出模拟物理世界的潜力。

本文基于公开的技术报告和逆向工程，对text-to-video模型的背景、相关技术、应用、存在的挑战以及未来的发展方向进行了全面的综述。

- 追溯了Sora的发展，并研究了用于构建这个“世界模拟器”的基础技术。
- 详细描述了Sora在从电影制作、教育到营销等多个行业的应用和潜在影响。
- 讨论了广泛部署 Sora 需要解决的主要挑战和限制。
- 讨论了Sora和视频生成模型的未来发展，如何实现人机交互的新方式。

# Abstact

区别于以往的视频生成技术，Sora 能够根据用户的文本指令，生成最长达一分钟的高清视频。
![Pasted image 20240304230213.png](/img/user/asserts/Pasted%20image%2020240304230213.png)
Sora 最令人瞩目的特点之一是它能够制作长达一分钟的视频，并且视频质量高、视觉连贯性强。不同于早期只能制作短片的模型，Sora 能够让视频从开始到结束都保持着视觉上的连贯性和故事进展。

**在技术层面，Sora 的核心是一种预先训练好的扩散式 Transformer。**
为了实现高效的视频生成，Sora 采用了*~~时空潜码补片~~(Spacetime Latent Patches)*作为其基本构成单元。
简而言之，Sora 将视频压缩为潜码的时空表示，然后从这个压缩的视频中提取出一系列的时空潜码补片，这些潜码补片概括了短时间内的视觉外观和运动动态。这些潜码补片，相当于语言模型中的词汇 Token，为 Sora 提供了构建视频的详细视觉“短语”。

Sora 利用扩散式 Transformer 模型，从一个充满视觉噪点的帧开始，逐步去噪并根据输入的文本提示添加具体细节，最终生成的视频经过多次精细化，更加符合预期的内容和质量。

> 在扩散模型中采用latent的概念来生成图像，一个观点认为，在一个维度较低的空间生成符合用户输入的图像特征，可以让模型更关注于这些特征提高生成的效果和细节。

**关于Sora的亮点**

- 提升模拟能力
	即便没有具体的3D模型，Sora 也能表现出3D世界的一致性，包括物体的持久存在和简单的世界互动，以及动态的摄像机移动和远景连贯性。
	
- 激发创造力
	Sora 加速了设计过程，让艺术家、电影制作人和设计师能够快速探索和精炼他们的创意，极大地激发了他们的创造潜能。
	
- 促进新兴应用的发展
	从营销人员使用它创建动态广告，到游戏开发者依据玩家的叙事生成定制化视觉效果或角色动作，Sora 都展现了强大的潜力。

**局限和机遇**
尽管 Sora 取得了显著的技术进步，但仍面临挑战，如更复杂动作的呈现和微妙面部表情的捕捉等。此外，确保生成内容无偏见且安全，避免不良视觉输出的伦理问题，也是开发者和研究者必须重视的。

<div style="width:100%; height:auto; max-width:100%; display:flex; justify-content:center; gap:5px">
	<video controls="" autoplay="true" preload="metadata" loop="true" style="width:50%; height:100%;">
		  <source id="mp4" src="https://cdn.openai.com/sora/videos/puppy-cloning.mp4" type="video/mp4">
	</video>
	<video controls="" autoplay="true" preload="metadata" loop="true" style="width:50%; height:100%;">
		<source id="mp4" src="https://cdn.openai.com/sora/videos/chair-archaeology.mp4" type="video/mp4">
	</video>
</div> 


# 2. Background

## 2.1 History


![Pasted image 20240304230228.png](/img/user/asserts/Pasted%20image%2020240304230228.png)
深度学习之前，人们主要依靠手工设计特征的方法来生成图像，比如纹理合成和纹理映射 。这些传统技术很难创造出既复杂又生动的图像。
生成对抗网络（GANs）和变分自编码器（VAEs）的出现成为了一个里程碑，它们在多个领域展现出了惊人的能力。
紧接着，流模型（flow model）和扩散模型的发展，使图像生成的细节和质量得到了进一步提升。

1. 在计算机视觉（CV）领域，研究者们更进一步，将 Transformer 架构与视觉元素相结合，使之能够应用于视觉领域的各种任务，如视觉 Transformer（ViT）和 Swin Transformer 所示。

2. 扩散模型在图像和视频生成领域也取得了显著的进展。扩散模型通过一个数学上的可靠框架，利用 U-Net 技术将噪声转化为图像，这一过程中，U-Net 通过预测和减少每步的噪声来帮助这一转换。

3. CLIP 是一个结合了 Transformer 架构和视觉元素的创新视觉-语言模型，它能够处理大量的文本和图像数据集。通过结合视觉和语言知识，CLIP 能够在多模态生成框架中作为图像编码器的角色。

![Pasted image 20240304232236.png](/img/user/asserts/Pasted%20image%2020240304232236.png)
![Pasted image 20240304232335.png](/img/user/asserts/Pasted%20image%2020240304232335.png)
![Pasted image 20240305101049.png](/img/user/asserts/Pasted%20image%2020240305101049.png)

**涌现性能力** 
大语言模型中的涌现性能力是指在模型达到一定规模时，出现的一些复杂行为或功能，这些并非开发者预先设定或预料的。根据 Sora 的技术报告，它是首个证实具有涌现性能力的视觉模型，为计算机视觉领域标记了一个重要的发展里程碑。

---


# 3. Technology

## 3.1 Overview of Sora
![Pasted image 20240304232848.png](/img/user/asserts/Pasted%20image%2020240304232848.png)

**Sora 本质上是一个具备灵活采样尺寸的视频生成技术，** 正如图 4 所示。它由三大核心部分组成：
1. 一个时间-空间编码器将原始视频转换为深层的潜在空间（latent space）表示;
2. 一个视觉Transformer (ViT) 处理这些潜在的数据表示，输出清洁、无噪声的视频数据表示
3. 一个类似于 CLIP 的智能条件设置机制利用大语言模型增强的用户指令和可能的视觉提示，引导视频生成过程，创造出具有特定风格或主题的视频

## 3.2 数据预处理

### 3.2.1 视频与图像多样性：时长、分辨率和宽高比

![Pasted image 20240304234337.png](/img/user/asserts/Pasted%20image%2020240304234337.png)

Sora 的一大特色就是它能够处理、理解并生成各种原生尺寸的视频和图像，正如图5所展示的。

> Sora 利用了diffusion transformer架构（详见第3.2.4节），它可以处理各种格式的视频和图像，从宽屏的1920x1080p到竖屏的1080x1920p，以及介于两者之间的任何尺寸，而不会改变它们的原始尺寸。

Sora 的训练方法遵循了 Richard Sutton 在《The Bitter Lesson》中提出的核心观点，即**优先利用计算力而不是人工设计的特性**，能够打造出更高效、更灵活的 AI 系统。

### 3.2.2 统一的视觉数据表现形式——patches

为了能够有效处理不同持续时间、分辨率和宽高比的图像和视频等多样化的视觉输入，一个关键策略是将这些不同形态的视觉数据转化为统一的格式。

具体而言，Sora 首先将视频数据压缩到一个更低维度的潜在空间中，接着再将这些数据分解为时空补丁（Spacetime Patches）。
Sora 的技术报告只是简略地介绍了这一概念，但这使得其他研究者难以实际操作实验。
![Pasted image 20240305010354.png](/img/user/asserts/Pasted%20image%2020240305010354.png)


### 3.2.3 视频压缩技术

![Pasted image 20240305010423.png](/img/user/asserts/Pasted%20image%2020240305010423.png)
Sora 的视频压缩技术旨在降低视频数据的维度，生成一个在时间和空间上都进行了压缩处理的潜在表示。根据技术报告中的引用，这一技术基于 VAE 或者向量量化的 VAE (VQ-VAE) 。

这论文作者介绍了两种可能的实现方法：

**空间patch压缩** ： 这项技术通过将视频帧分割成固定大小的区块（Patches），然后将这些区块编码到一个隐藏的空间中，从而处理视频。接下来，这些所谓的空间标记按时间顺序排列，形成一个结合了空间和时间的隐藏表示。

视频时长的变化意味着隐藏空间的时间维度不能固定。解决方案包括选取特定数量的帧（较短视频可能需要加入额外帧或进行时间插值），或者定义一个超长的输入长度以便后续处理；

Sora 的团队则计划从零开始，自行训练一个包含解码器的压缩网络，后者负责生成视频，这一过程借鉴了训练潜码扩散模型的方法。

**时间-空间patch压缩**：这一技术致力于同时封装视频数据的空间和时间维度，以提供一个全方位的表述。

会由于视频输入的特性差异，导致潜在空间维度的不同。为应对这一挑战，**空间划分**的方法同样适用且有效。
![Pasted image 20240305102134.png](/img/user/asserts/Pasted%20image%2020240305102134.png)

### 3.2.4 spacetime latent patches

在视频压缩网络的设计中，我们面临一个关键挑战：**如何在输入层处理来自不同视频类型的潜在特征块或patch数量的差异**

根据 Sora 的技术报告及相关文献，一种被称为打包与封装 (PNP) 的方法显得尤为合适。

![Pasted image 20240305102623.png](/img/user/asserts/Pasted%20image%2020240305102623.png)
这种方法借鉴了自然语言处理中对变长输入进行高效训练的示例打包技术，通过舍弃部分词元来适应输入长度的变化。

因此，作责认为 OpenAI 可能采用了一个极长的处理窗口来整合视频中所有的信息patches（Token），虽然这样的处理方式计算成本高昂。

作者认为Sora 首先将视觉patches压缩成低维度的隐藏表示，然后将这些表示或进一步处理的patches以序列形式组织起来，添加噪声，输入到扩散transformer。

## 3.3 Modeling

### 3.3.1 Diffusion Transformer
![Pasted image 20240305103456.png](/img/user/asserts/Pasted%20image%2020240305103456.png)
**Image Diffusion Transformer.** 
传统的扩散模型主要依赖于包括降低和提高图像分辨率的处理块的卷积U-Net架构，作为其去噪网络的核心。

通过引入更加灵活的Transformer架构，基于Transformer的扩散模型能够处理更多的训练数据并支持更大的模型参数。

DiT和U-ViT是首批采用视觉Transformer技术构建潜在扩散模型的先行者。

**Video Diffusion Transformer.** 
基于文本到图像转换（T2I）扩散模型的基础性研究，近期研究主要致力于探索扩散转换器在文本到视频生成（T2V）任务中的应用潜力。

视频的时空特性给 DiTs 在视频领域的应用带来了三大挑战：
- 一是如何在空间和时间上有效压缩视频到潜码空间进行高效去噪；
- 二是如何将这些压缩后的潜在信息转换成小块（patches）并输入到变换器中；
- 三是如何处理视频长期的时空依赖性并保证内容连贯性。

*Imagen Video*
![Pasted image 20240305105002.png](/img/user/asserts/Pasted%20image%2020240305105002.png)
这是谷歌推出的视频生成模型；
这个流程首先通过一个固定的 T5 文本编码器，将文本提示转化为深层次的上下文信息；
然后，这些深层信息被整合到后续所有处理步骤中，包括基础的视频生成过程。
接下来，这个基础模型先产生一个低分辨率的视频，之后再通过一系列精细的模型处理，逐步提升视频的清晰度。

在这个转换过程中，Imagen Video 采用了一种特别的3D U-Net架构，这种设计巧妙地结合了时间和空间处理，以高效捕捉视频帧之间的动态关系。

*Video LDM*
![Pasted image 20240305105143.png](/img/user/asserts/Pasted%20image%2020240305105143.png)
Blattmann 和团队提出了一个创新思路: 将传统的二维潜码扩散模型改进为能处理视频内容的视频潜码扩散模型。

通过在 U-Net 结构的基础上和 VAE 解码器中增加特定的时间处理层来实现这一目标，这些层专门用来整合和对齐视频帧。

### 3.3.2 Discussion

**Cascade diffusion models for spatial and temporal up-sampling.**
作者推测Sora采用了一种特殊的模型架构，称为级联扩散模型。

这种架构包括一个基本模型和多个用于细化空间和时间的模型。在这个体系中，基础模型和低分辨率模型可能不会大量使用注意力机制。

为了保证视频和场景在空间和时间上的连贯性，Sora 更注重时间连贯性而非空间连贯性，因为研究显示时间连贯性对视频或场景的生成更为关键。因此，Sora 可能采用了一种高效的训练策略，使用时间较长但分辨率较低的视频来实现时间上的连贯性。

此外，考虑到其优越的性能，Sora 可能使用了一种特殊的**v-参数化扩散模型**，这种模型在预测原始潜码变量x或噪声ϵ方面比其他模型更为出色。

**On the latent encoder**
为了提高训练效率，许多现有的研究选择使用预训练的稳定扩散 VAE 编码器作为模型训练的起点。但这些编码器缺少处理视频时间信息的能力。

根据技术报告，与其使用预训练的 VAE 编码器，Sora 更可能采用从头开始训练的空间-时间 VAE 编码器，这种编码器针对视频数据进行优化，其性能超越了现有技术，特别是在处理视频压缩潜码空间方面。

---
## 3.4 Language Instruction Following

**用户通常通过输入自然语言的指令来与生成式 AI 模型互动，这些指令也就是我们所说的文本提示。** 
Sora 在提升文本到视频模型理解文本指令的能力方面，采取了与 DALL·E 3 相似的策略，通过训练一个专门的描述性字幕制作器，并利用其生成的数据来进行模型的微调。

### 3.4.2 text-to-imge
**DALL·E 3 所采用的策略基于一个核心假设**：模型训练所用的文本-图片对质量直接影响到最终生成的文字到图片模型的表现。数据质量低下，尤其是充斥着的噪声数据和缺少大量视觉信息的简短标题，会引起诸如忽略关键词、混淆词序以及误解用户意图等一系列问题 。

为了解决这些问题，提出了一种通过**为现有图片重新编写更详尽描述性的标题**的方法。该过程首先是训练一个能生成精确描述性图像标题的视觉-语言模型（CLIP）。随后，这些生成的描述性图像标题被用于微调文字到图片模型。

> **Sora 能够根据用户的指令创造出长达一分钟、场景复杂且符合用户意图的视频，这种指令执行能力至关重要**。Sora 的技术报告 [3] 透露，这一能力是通过开发一个能生成详尽字幕的系统获得的，这些详尽的字幕随后被用来训练模型。但是，如何收集用于训练此系统的数据仍是一个谜，这个过程可能非常耗时，因为它需要对视频内容进行详细的描述。此外，视频描述系统有时可能会错误地添加视频中并不存在的细节。我们认为，改进视频描述系统，以更好地遵循指令，是一个值得进一步探究的关键问题。

## 3.5 提示工程

参考技术报告中的视频

## 3.6 可信性

随着 ChatGPT、GPT4-V 和 Sora 等高级模型的快速进步，它们的功能已经得到了极大的增强，为提高工作效率和促进技术革新作出了显著贡献。但是，这些进步同时也带来了一系列问题，如假新闻的产生、隐私泄漏以及伦理道德的挑战。因此，如何确保这些强大的模型可靠且不被滥用，已经成为了学术界和产业界共同关注的重点议题。

### 3.6.1 安全问题

模型的安全性是一个重点关注领域，特别是其在面对误用和“越狱”攻击的抵抗能力方面。越狱攻击指用户尝试通过漏洞生成违禁或有害内容的行为。

除了文本攻击，对于多模态模型（如 GPT-4V 和 Sora）来说，视觉越狱同样构成安全威胁。

### 3.6.2 其他问题

“虚假输出”在此背景下，指的是模型生成的回答可能听起来令人信服，但实际上是没有依据或是错误的。这一现象对模型输出的可靠性和信赖度提出了挑战，迫切需要采取全面措施来评价并解决此问题。


# 4 应用领域

![Pasted image 20240305121146.png](/img/user/asserts/Pasted%20image%2020240305121146.png)
## 4.1 电影产业

研究人员利用视频生成模型拓展到电影制作领域，开创了电影生成的新篇章。例如，MovieFactory 利用扩散模型根据 ChatGPT生成的精细剧本生成电影风格的视频，标志着技术上的一大进步。随后，MobileVidFactory 能够仅凭用户提供的简文本自动创作竖屏移动视频。Vlogger 让用户能以此技术创作出一分钟长的视频vlog。

## 4.2 教育革新

**长期以来，教育领域的内容主要由静态资源构成，虽然这些资源具有一定的价值，但它们往往无法满足当前学生的多元化需求和学习方式**。

图像至视频的编辑技巧为将静态教育资源变为互动视频提供了创新方法，满足了各种学习偏好，有望进一步提升学生的参与感。

## 4.3 游戏行业

**游戏产业始终在寻找方法，以突破真实感和沉浸体验的边界。**

传统的游戏开发往往受限于预设的环境和剧本事件。现在，利用扩散模型实时生成的动态高清视频内容和逼真音效，有望突破这些限制。

## 4.4 医疗保健

**在医疗保健领域，尽管主要强调创造能力，视频扩散模型在理解和生成复杂视频序列方面的能力，使其特别适合于识别身体内部的动态变化，如细胞早期的自我消亡、皮肤病变的发展以及不规则的人体运动**

通过将 Sora 技术融入临床实践，不仅可以优化诊断流程，还可以根据精确的医学成像分析，为患者提供定制化的治疗方案。然而，技术的融合也带来了挑战，包括必须建立强有力的数据隐私保护措施，并在医疗实践中考虑伦理问题。

## 4.5 机器人

利用视频扩散模型创造出的高度逼真的视频序列，解决了机器人研究依赖模拟环境的局限性，为机器人提供了丰富多样的训练场景，克服了真实世界数据不足的问题。

## 5 局限性

- **面对物理真实性的挑战，Sora 作为一个仿真平台，在准确再现复杂情境方面存在一些局限。**

- **空间和时间方面的复杂性也是一个挑战**

- **在人机交互（HCI）方面，尽管Sora 在视频生成领域展现了潜力，但它在 HCI 方面存在显著的限制**。
	Sora 在理解复杂的语言指令或把握细微的语义差异方面也显示出限制，可能导致视频内容无法完全满足用户的期望或需求。



